{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to the exercise about observational studies! This exercise will be hands on, and you will be able to practise the skills you developed so far!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity score matching\n",
    "\n",
    "In this exercise, you will apply [propensity score matching](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf), which we discussed in lecture 6 (\"Observational studies\"), in order to draw conclusions from an observational study.\n",
    "\n",
    "We will work with a by-now classic dataset from Robert LaLonde's study \"[Evaluating the Econometric Evaluations of Training Programs](http://people.hbs.edu/nashraf/LaLonde_1986.pdf)\" (1986).\n",
    "The study investigated the effect of a job training program (\"National Supported Work Demonstration\") on the real earnings of an individual, a couple of years after completion of the program.\n",
    "Your task is to determine the effectiveness of the \"treatment\" represented by the job training program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset description\n",
    "\n",
    "- `treat`: 1 if the subject participated in the job training program, 0 otherwise\n",
    "- `age`: the subject's age\n",
    "- `educ`: years of education\n",
    "- `race`: categorical variable with three possible values: Black, Hispanic, or White\n",
    "- `married`: 1 if the subject was married at the time of the training program, 0 otherwise\n",
    "- `nodegree`: 1 if the subject has earned no school degree, 0 otherwise\n",
    "- `re74`: real earnings in 1974 (pre-treatment)\n",
    "- `re75`: real earnings in 1975 (pre-treatment)\n",
    "- `re78`: real earnings in 1978 (outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to deepen your knowledge on propensity scores and observational studies, we highly recommend Rosenbaum's excellent book on the [\"Design of Observational Studies\"](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf). Even just reading the first chapter (18 pages) will help you a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "\n",
    "data_folder = './data/'\n",
    "df = pd.read_csv(data_folder + 'lalonde.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          id  treat  age  educ  black  hispan  married  nodegree       re74  \\\n",
       "148   NSW149      1   26    11      1       0        1         1     0.0000   \n",
       "379  PSID195      0   32     4      0       0        1         1     0.0000   \n",
       "380  PSID196      0   18    11      1       0        0         1     0.0000   \n",
       "555  PSID371      0   26    14      0       0        0         0     0.0000   \n",
       "451  PSID267      0   21    14      0       0        0         0   107.7597   \n",
       "604  PSID420      0   39     2      1       0        1         1     0.0000   \n",
       "437  PSID253      0   21     9      1       0        0         1  1030.5740   \n",
       "143   NSW144      1   46     8      1       0        0         1  3165.6580   \n",
       "495  PSID311      0   55     7      0       0        1         1  8832.3750   \n",
       "115   NSW116      1   21    12      0       0        0         0  3670.8720   \n",
       "\n",
       "          re75         re78  \n",
       "148  2754.6460  26372.28000  \n",
       "379  1378.5480      0.00000  \n",
       "380  1367.8060     33.98771  \n",
       "555     0.0000   6717.74500  \n",
       "451   293.6129   7698.95500  \n",
       "604     0.0000    964.95550  \n",
       "437   470.8548   1223.55800  \n",
       "143  2594.7230      0.00000  \n",
       "495     0.0000      0.00000  \n",
       "115   334.0494  12558.02000  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>treat</th>\n      <th>age</th>\n      <th>educ</th>\n      <th>black</th>\n      <th>hispan</th>\n      <th>married</th>\n      <th>nodegree</th>\n      <th>re74</th>\n      <th>re75</th>\n      <th>re78</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>148</th>\n      <td>NSW149</td>\n      <td>1</td>\n      <td>26</td>\n      <td>11</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>2754.6460</td>\n      <td>26372.28000</td>\n    </tr>\n    <tr>\n      <th>379</th>\n      <td>PSID195</td>\n      <td>0</td>\n      <td>32</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>1378.5480</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>380</th>\n      <td>PSID196</td>\n      <td>0</td>\n      <td>18</td>\n      <td>11</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>1367.8060</td>\n      <td>33.98771</td>\n    </tr>\n    <tr>\n      <th>555</th>\n      <td>PSID371</td>\n      <td>0</td>\n      <td>26</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>6717.74500</td>\n    </tr>\n    <tr>\n      <th>451</th>\n      <td>PSID267</td>\n      <td>0</td>\n      <td>21</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>107.7597</td>\n      <td>293.6129</td>\n      <td>7698.95500</td>\n    </tr>\n    <tr>\n      <th>604</th>\n      <td>PSID420</td>\n      <td>0</td>\n      <td>39</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>964.95550</td>\n    </tr>\n    <tr>\n      <th>437</th>\n      <td>PSID253</td>\n      <td>0</td>\n      <td>21</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1030.5740</td>\n      <td>470.8548</td>\n      <td>1223.55800</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>NSW144</td>\n      <td>1</td>\n      <td>46</td>\n      <td>8</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3165.6580</td>\n      <td>2594.7230</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>PSID311</td>\n      <td>0</td>\n      <td>55</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>8832.3750</td>\n      <td>0.0000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>NSW116</td>\n      <td>1</td>\n      <td>21</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3670.8720</td>\n      <td>334.0494</td>\n      <td>12558.02000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A naive analysis\n",
    "\n",
    "Compare the distribution of the outcome variable (`re78`) between the two groups, using plots and numbers.\n",
    "To summarize and compare the distributions, you may use the techniques we discussed in lecture 4 (\"Descibing data\") and 3 (\"Visualizing data\").\n",
    "\n",
    "What might a naive \"researcher\" conclude from this superficial analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' your code and explanations ''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A closer look at the data\n",
    "\n",
    "You're not naive, of course (and even if you are, you've learned certain things in ADA), so you aren't content with a superficial analysis such as the above.\n",
    "You're aware of the dangers of observational studies, so you take a closer look at the data before jumping to conclusions.\n",
    "\n",
    "For each feature in the dataset, compare its distribution in the treated group with its distribution in the control group, using plots and numbers.\n",
    "As above, you may use the techniques we discussed in class for summarizing and comparing the distributions.\n",
    "\n",
    "What do you observe?\n",
    "Describe what your observations mean for the conclusions drawn by the naive \"researcher\" from his superficial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' your code and explanations ''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A propensity score model\n",
    "\n",
    "Use logistic regression to estimate propensity scores for all points in the dataset.\n",
    "You may use `statsmodels` to fit the logistic regression model and apply it to each data point to obtain propensity scores.\n",
    "\n",
    "Recall that the propensity score of a data point represents its probability of receiving the treatment, based on its pre-treatment features (in this case, age, education, pre-treatment income, etc.).\n",
    "To brush up on propensity scores, you may read chapter 3.3 of the above-cited book by Rosenbaum or [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).\n",
    "\n",
    "Note: you do not need a train/test split here. Train and apply the model on the entire dataset. If you're wondering why this is the right thing to do in this situation, recall that the propensity score model is not used in order to make predictions about unseen data. Its sole purpose is to balance the dataset across treatment groups.\n",
    "(See p. 74 of Rosenbaum's book for an explanation why slight overfitting is even good for propensity scores.\n",
    "If you want even more information, read [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimization terminated successfully.\n         Current function value: 0.397267\n         Iterations 7\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                  treat   No. Observations:                  614\nModel:                          Logit   Df Residuals:                      605\nMethod:                           MLE   Df Model:                            8\nDate:                Fri, 08 Jan 2021   Pseudo R-squ.:                  0.3508\nTime:                        19:46:43   Log-Likelihood:                -243.92\nconverged:                       True   LL-Null:                       -375.75\nCovariance Type:            nonrobust   LLR p-value:                 2.194e-52\n====================================================================================\n                       coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept           -2.8509      0.350     -8.147      0.000      -3.537      -2.165\nC(black)[T.1]        3.0654      0.287     10.698      0.000       2.504       3.627\nC(hispan)[T.1]       0.9836      0.426      2.311      0.021       0.149       1.818\nC(married)[T.1]     -0.8321      0.290     -2.866      0.004      -1.401      -0.263\nC(nodegree)[T.1]     0.7073      0.338      2.095      0.036       0.045       1.369\nage                  0.1559      0.134      1.162      0.245      -0.107       0.419\neduc                 0.4240      0.171      2.477      0.013       0.088       0.759\nre74                -0.4650      0.186     -2.497      0.013      -0.830      -0.100\nre75                 0.1761      0.153      1.153      0.249      -0.123       0.476\n====================================================================================\n"
     ]
    }
   ],
   "source": [
    "# let's standardize the continuous features\n",
    "df['age'] = (df['age'] - df['age'].mean())/df['age'].std()\n",
    "df['educ'] = (df['educ'] - df['educ'].mean())/df['educ'].std()\n",
    "df['re74'] = (df['re74'] - df['re74'].mean())/df['re74'].std()\n",
    "df['re75'] = (df['re75'] - df['re75'].mean())/df['re75'].std()\n",
    "\n",
    "mod = smf.logit(formula='treat ~ age + educ + C(black) + C(hispan)  + C(married) + C(nodegree) + +re74 + re75', data=df)\n",
    "\n",
    "res = mod.fit()\n",
    "\n",
    "# Extract the estimated propensity scores\n",
    "df['Propensity_score'] = res.predict()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Balancing the dataset via matching\n",
    "\n",
    "Use the propensity scores to match each data point from the treated group with exactly one data point from the control group, while ensuring that each data point from the control group is matched with at most one data point from the treated group.\n",
    "(Hint: you may explore the `networkx` package in Python for predefined matching functions.)\n",
    "\n",
    "Your matching should maximize the similarity between matched subjects, as captured by their propensity scores.\n",
    "In other words, the sum (over all matched pairs) of absolute propensity-score differences between the two matched subjects should be minimized.\n",
    "\n",
    "This is how networkx library can help you do this. Each possible pair of (treated_person, control_person) is characterized by a similarity. This is how we can initialize a graph, and add an edge for one possible pair. We then need to add an edge for each possible pair.\n",
    "    - G = nx.Graph()\n",
    "    - G.add_weighted_edges_from([(control_person, treated_person, similarity)])\n",
    "Optimal matching is then found with:\n",
    "    - matching = nx.max_weight_matching(G)\n",
    "\n",
    "After matching, you have as many treated as you have control subjects.\n",
    "Compare the outcomes (`re78`) between the two groups (treated and control).\n",
    "\n",
    "Also, compare again the feature-value distributions between the two groups, as you've done in part 2 above, but now only for the matched subjects.\n",
    "What do you observe?\n",
    "Are you closer to being able to draw valid conclusions now than you were before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similarity(propensity_score1, propensity_score2):\n",
    "    '''Calculate similarity for instances with given propensity scores'''\n",
    "    return 1-np.abs(propensity_score1-propensity_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the treatment and control groups\n",
    "treatment_df = df[df['treat'] == 1]\n",
    "control_df = df[df['treat'] == 0]\n",
    "\n",
    "# Create an empty undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Loop through all the pairs of instances\n",
    "for control_id, control_row in control_df.iterrows():\n",
    "    for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "\n",
    "        # Calculate the similarity \n",
    "        similarity = get_similarity(control_row['Propensity_score'],\n",
    "                                    treatment_row['Propensity_score'])\n",
    "\n",
    "        # Add an edge between the two instances weighted by the similarity between them\n",
    "        G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "\n",
    "# Generate and return the maximum weight matching on the generated graph\n",
    "matching = nx.max_weight_matching(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            treat         age        educ       black      hispan     married  \\\n",
       "count  370.000000  370.000000  370.000000  370.000000  370.000000  370.000000   \n",
       "mean     0.500000   -0.182542    0.078737    0.656757    0.137838    0.200000   \n",
       "std      0.500677    0.913502    0.896839    0.475435    0.345197    0.400542   \n",
       "min      0.000000   -1.149982   -3.526478    0.000000    0.000000    0.000000   \n",
       "25%      0.000000   -0.846375   -0.482714    0.000000    0.000000    0.000000   \n",
       "50%      0.500000   -0.441566    0.278227    1.000000    0.000000    0.000000   \n",
       "75%      1.000000    0.165649    0.658697    1.000000    0.000000    0.000000   \n",
       "max      1.000000    2.796912    2.941520    1.000000    1.000000    1.000000   \n",
       "\n",
       "         nodegree        re74        re75          re78  Propensity_score  \n",
       "count  370.000000  370.000000  370.000000    370.000000        370.000000  \n",
       "mean     0.672973   -0.361025   -0.185558   5901.959790          0.470163  \n",
       "std      0.469762    0.705428    0.891102   7028.505472          0.260227  \n",
       "min      0.000000   -0.703546   -0.662971      0.000000          0.024896  \n",
       "25%      0.000000   -0.703546   -0.662971     31.771122          0.177684  \n",
       "50%      1.000000   -0.703546   -0.654822   3886.808000          0.581370  \n",
       "75%      1.000000   -0.402532   -0.089452   9082.846000          0.697506  \n",
       "max      1.000000    4.705571    6.965879  60307.930000          0.853153  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>treat</th>\n      <th>age</th>\n      <th>educ</th>\n      <th>black</th>\n      <th>hispan</th>\n      <th>married</th>\n      <th>nodegree</th>\n      <th>re74</th>\n      <th>re75</th>\n      <th>re78</th>\n      <th>Propensity_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.500000</td>\n      <td>-0.182542</td>\n      <td>0.078737</td>\n      <td>0.656757</td>\n      <td>0.137838</td>\n      <td>0.200000</td>\n      <td>0.672973</td>\n      <td>-0.361025</td>\n      <td>-0.185558</td>\n      <td>5901.959790</td>\n      <td>0.470163</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.500677</td>\n      <td>0.913502</td>\n      <td>0.896839</td>\n      <td>0.475435</td>\n      <td>0.345197</td>\n      <td>0.400542</td>\n      <td>0.469762</td>\n      <td>0.705428</td>\n      <td>0.891102</td>\n      <td>7028.505472</td>\n      <td>0.260227</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>-1.149982</td>\n      <td>-3.526478</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.703546</td>\n      <td>-0.662971</td>\n      <td>0.000000</td>\n      <td>0.024896</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>-0.846375</td>\n      <td>-0.482714</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.703546</td>\n      <td>-0.662971</td>\n      <td>31.771122</td>\n      <td>0.177684</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.500000</td>\n      <td>-0.441566</td>\n      <td>0.278227</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.703546</td>\n      <td>-0.654822</td>\n      <td>3886.808000</td>\n      <td>0.581370</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>0.165649</td>\n      <td>0.658697</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.402532</td>\n      <td>-0.089452</td>\n      <td>9082.846000</td>\n      <td>0.697506</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>2.796912</td>\n      <td>2.941520</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>4.705571</td>\n      <td>6.965879</td>\n      <td>60307.930000</td>\n      <td>0.853153</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "matching = np.array(list(matching))\n",
    "balanced_df_1 = df.iloc[matching.flatten()]\n",
    "balanced_df_1.describe()\n",
    "# race not balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Balancing the groups further\n",
    "\n",
    "Based on your comparison of feature-value distributions from part 4, are you fully satisfied with your matching?\n",
    "Would you say your dataset is sufficiently balanced?\n",
    "If not, in what ways could the \"balanced\" dataset you have obtained still not allow you to draw valid conclusions?\n",
    "\n",
    "Improve your matching by explicitly making sure that you match only subjects that have the same value for the problematic feature.\n",
    "Argue with numbers and plots that the two groups (treated and control) are now better balanced than after part 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an empty undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Loop through all the pairs of instances\n",
    "for control_id, control_row in control_df.iterrows():\n",
    "    for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "\n",
    "        # Calculate the similarity \n",
    "        if control_row['black'] == treatment_row['black'] and \\\n",
    "            control_row['hispan'] == treatment_row['hispan']:\n",
    "            similarity = get_similarity(control_row['Propensity_score'],\n",
    "                                        treatment_row['Propensity_score'])\n",
    "\n",
    "        # Add an edge between the two instances weighted by the similarity between them\n",
    "        G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "\n",
    "# Generate and return the maximum weight matching on the generated graph\n",
    "matching2 = nx.max_weight_matching(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            treat         age        educ       black      hispan     married  \\\n",
       "count  370.000000  370.000000  370.000000  370.000000  370.000000  370.000000   \n",
       "mean     0.500000   -0.177619    0.073595    0.605405    0.083784    0.202703   \n",
       "std      0.500677    0.915945    0.864701    0.489425    0.277438    0.402557   \n",
       "min      0.000000   -1.149982   -3.526478    0.000000    0.000000    0.000000   \n",
       "25%      0.000000   -0.846375   -0.482714    0.000000    0.000000    0.000000   \n",
       "50%      0.500000   -0.441566    0.278227    1.000000    0.000000    0.000000   \n",
       "75%      1.000000    0.165649    0.658697    1.000000    0.000000    0.000000   \n",
       "max      1.000000    2.796912    2.941520    1.000000    1.000000    1.000000   \n",
       "\n",
       "         nodegree        re74        re75          re78  Propensity_score  \n",
       "count  370.000000  370.000000  370.000000    370.000000        370.000000  \n",
       "mean     0.656757   -0.343267   -0.187089   5686.315600          0.439029  \n",
       "std      0.475435    0.697757    0.874578   6858.134154          0.285185  \n",
       "min      0.000000   -0.703546   -0.662971      0.000000          0.015487  \n",
       "25%      0.000000   -0.703546   -0.662971     39.159753          0.104865  \n",
       "50%      1.000000   -0.703546   -0.614819   3747.122000          0.569317  \n",
       "75%      1.000000   -0.296844    0.013151   8895.676500          0.695175  \n",
       "max      1.000000    4.705571    6.965879  60307.930000          0.853153  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>treat</th>\n      <th>age</th>\n      <th>educ</th>\n      <th>black</th>\n      <th>hispan</th>\n      <th>married</th>\n      <th>nodegree</th>\n      <th>re74</th>\n      <th>re75</th>\n      <th>re78</th>\n      <th>Propensity_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n      <td>370.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.500000</td>\n      <td>-0.177619</td>\n      <td>0.073595</td>\n      <td>0.605405</td>\n      <td>0.083784</td>\n      <td>0.202703</td>\n      <td>0.656757</td>\n      <td>-0.343267</td>\n      <td>-0.187089</td>\n      <td>5686.315600</td>\n      <td>0.439029</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.500677</td>\n      <td>0.915945</td>\n      <td>0.864701</td>\n      <td>0.489425</td>\n      <td>0.277438</td>\n      <td>0.402557</td>\n      <td>0.475435</td>\n      <td>0.697757</td>\n      <td>0.874578</td>\n      <td>6858.134154</td>\n      <td>0.285185</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>-1.149982</td>\n      <td>-3.526478</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.703546</td>\n      <td>-0.662971</td>\n      <td>0.000000</td>\n      <td>0.015487</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>-0.846375</td>\n      <td>-0.482714</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.703546</td>\n      <td>-0.662971</td>\n      <td>39.159753</td>\n      <td>0.104865</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.500000</td>\n      <td>-0.441566</td>\n      <td>0.278227</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.703546</td>\n      <td>-0.614819</td>\n      <td>3747.122000</td>\n      <td>0.569317</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>0.165649</td>\n      <td>0.658697</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-0.296844</td>\n      <td>0.013151</td>\n      <td>8895.676500</td>\n      <td>0.695175</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>2.796912</td>\n      <td>2.941520</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>4.705571</td>\n      <td>6.965879</td>\n      <td>60307.930000</td>\n      <td>0.853153</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "matching2 = np.array(list(matching2))\n",
    "balanced_df_2 = df.iloc[matching2.flatten()]\n",
    "balanced_df_2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. A less naive analysis\n",
    "\n",
    "Compare the outcomes (`re78`) between treated and control subjects, as you've done in part 1, but now only for the matched dataset you've obtained from part 5.\n",
    "What do you conclude about the effectiveness of the job training program?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' your code and explanations ''';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}